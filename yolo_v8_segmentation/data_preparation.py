# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_data_preparation.ipynb.

# %% auto 0
__all__ = ['get_name', 'get_contours', 'from_contr_to_annotation', 'get_mask_info', 'process_masks', 'read_json', 'get_file_info',
           'get_annotations', 'normalized_polygon', 'create_yolo_dataset', 'create_yaml']

# %% ../nbs/01_data_preparation.ipynb 1
from pathlib import Path
from fastcore.all import *
import cv2
import numpy as np
from typing import List, Tuple, Union, Callable, Optional, Dict
from tqdm.auto import tqdm
import matplotlib.pyplot as plt 
import json
import yaml

# %% ../nbs/01_data_preparation.ipynb 9
get_name = np.vectorize(lambda x: Path(x).name)

# %% ../nbs/01_data_preparation.ipynb 10
def get_contours(img:np.ndarray):
    'get contours from masks'

    _, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    return cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[0]


# %% ../nbs/01_data_preparation.ipynb 11
def from_contr_to_annotation(
                            sn_cntr:list, # single contour
                            consider_min_area:bool=True,# whether to use min_area parameter
                            min_area:int=0,
                            )->Tuple:
    'Create annotation dict from  a single contour'
    bbox = cv2.boundingRect(sn_cntr)
    area = cv2.contourArea(sn_cntr)
    segmentation = sn_cntr.flatten().tolist()
    if consider_min_area:
        if area > min_area:
            return bbox, area, segmentation
        return None, None, None
    else:
        return bbox, area, segmentation



# %% ../nbs/01_data_preparation.ipynb 12
def get_mask_info(
        msk_path, 
        min_area=0,
        )->Tuple:
    all_masks = msk_path.ls()

    image_infos = []
    annotations = []
    annotation_id=0
    for idx, msk_fn in tqdm(enumerate(all_masks),total=len(all_masks)):
        image_id = idx +1
        file_name = msk_fn.name

        mask = cv2.imread(str(msk_fn), cv2.IMREAD_GRAYSCALE)
        height, width = mask.shape

        if file_name not in map(itemgetter('file_name'), image_infos):  
            image_info = {
                'id': image_id,
                'width': width,
                'height': height,
                'file_name': file_name}
            image_infos.append(image_info)
        else:
            image_info = list(filter(lambda x: x['file_name'] == file_name, image_infos))[0]
        
        cntrs = get_contours(mask)
        for cntr in cntrs:
            bbox, area, segmentation = from_contr_to_annotation(cntr, min_area=min_area)
            if bbox:
                annotation = {
                    'image_id': image_id,
                    'id': annotation_id,
                    'category_id': 1,
                    'iscrowd': 0,
                    'area': area,
                    'bbox': bbox,
                    'segmentation': [segmentation]
                }
                annotations.append(annotation)
                annotation_id +=1
    return image_infos, annotations, annotation_id


        

        



# %% ../nbs/01_data_preparation.ipynb 15
def process_masks(
        mask_path:Union[str, Path],
        json_path:Union[str, Path],
        category_ids:Dict,
        ):
    coco_format = {
        "info": {},
        "licenses": [],
        "images":[],
        "categories": [{"id":v, "name":k, "supercategory":k } for k, v in category_ids.items()],
        "annotations":[]

    }

    coco_format['images'], coco_format['annotations'], ann_cnt = get_mask_info(mask_path)
    with open(json_path, 'w') as f:
        json.dump(coco_format, f, sort_keys=True, indent=4)

# %% ../nbs/01_data_preparation.ipynb 21
def read_json(file_path):
    with open(file_path, 'r') as f:
        data = json.load(f)
    return data


# %% ../nbs/01_data_preparation.ipynb 26
def get_file_info(json_data:dict, file_name:str):
    return list(filter(lambda x: x['file_name'] == file_name, json_data['images']))[0]

# %% ../nbs/01_data_preparation.ipynb 27
def get_annotations(json_data:dict, file_name:str):
    image_id = get_file_info(json_data, file_name)['id']
    return list(filter(lambda x: x['image_id'] == image_id, json_data['annotations']))

# %% ../nbs/01_data_preparation.ipynb 28
def normalized_polygon(polygon:List, width:int, height:int):
    'normalize polygon coordinates based on image height and width'

    n_p = np.array(polygon).reshape(-1, 2) / np.array([width, height])
    return n_p.flatten().tolist()



# %% ../nbs/01_data_preparation.ipynb 30
def create_yolo_dataset(
    img_path:Union[str, Path],
    output_path:Union[str, Path],
    json_path:Union[str, Path],
    )->None:

    'Create yolo dataset from coco format'

    Path(output_path).mkdir(parents=True, exist_ok=True)

    json_data = read_json(json_path)

    # getting the names of  the images
    image_names = get_name(img_path.ls())

    for i in tqdm(image_names,total=len(image_names)):

        file_info = get_file_info(json_data, i)
        image_height = file_info['height']
        image_width = file_info['width']
        image_annotation = get_annotations(json_data, i)
        # in case annotations available
        if image_annotation:

            # Creating txt file for each image
            with open(output_path/f'{Path(i).stem}.txt', 'w') as f_o:
                for ann in image_annotation:
                    current_cat = ann['category_id'] -1
                    polygon = ann['segmentation'][0]
                    norm_poly = normalized_polygon(
                                                polygon, 
                                                width=image_width, 
                                                height=image_height)

                    f_o.write(f'{current_cat} {" ".join(map(str, norm_poly))}\n')


# %% ../nbs/01_data_preparation.ipynb 34
def create_yaml(
    json_path:Union[str, Path], # json path with its name
    yaml_path:Union[str, Path], # output path with yaml name
    train_path:Union[str, Path],# train images path
    val_path:Union[str, Path], # validation images path
    test_path:Union[str, Path, None]=None,
    )->None:

    ' Create a yaml with trianing and validation images path'


    json_data = read_json(json_path)
    names = [cat['name']for cat in json_data['categories']]

    # Number of classes
    nc = len(names)
    yaml_data ={
        'names': names,
        'nc': nc,
        'test':str(test_path) if test_path else '',
        'train':str(train_path),
         'val':str(val_path)

    }
    with open(yaml_path, 'w') as f:
        yaml.dump(
                yaml_data, 
                f, 
                default_flow_style=False
                )

